{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based versus model-free learning project: Forward learning\n",
    "<font color=red>Nothing done.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# define constants for actions and transition type\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "COMMON = 0\n",
    "RARE = 1\n",
    "\n",
    "# inverse temperature\n",
    "tau = 10\n",
    "\n",
    "# the matrix R defines the probability of reward on each trial for each action\n",
    "R = np.zeros( (3,2,300) )\n",
    "R[1:,:,0] = np.random.rand(2,2) * .5 + .25\n",
    "for trial in range(1, 300):\n",
    "    # write function to create random walk for each state-action\n",
    "    for s in range(1,3):\n",
    "        for a in range(2):\n",
    "            R[s,a,trial] = R[s,a,trial-1] + np.random.normal(0,.025)\n",
    "            if R[s,a,trial] < .25:\n",
    "                R[s,a,trial] += 2 * (.25-R[s,a,trial])\n",
    "            elif R[s,a,trial] > .75:\n",
    "                R[s,a,trial] -= 2 * (R[s,a,trial]-.75)\n",
    "\n",
    "# next_state returns the stage 2 state and whether the transition was COMMON or\n",
    "# RARE in tuple: (s2, COMMON/RARE) based on action taken in stage 1\n",
    "def next_state(action):\n",
    "    common_rare = np.random.choice([COMMON, RARE], p=[.7, .3])\n",
    "    \n",
    "    if (action==LEFT and common_rare==COMMON) or (action==RIGHT and common_rare==RARE):\n",
    "        return (1, common_rare)\n",
    "    else:\n",
    "        return (2, common_rare)\n",
    "    \n",
    "# softmax decision function returns probability of choosing action with value Q\n",
    "# in a two-alternative choice with value of the alternate action given by Qalt\n",
    "def softmax(Q, Qalt):\n",
    "    global tau\n",
    "    numerator = math.exp(tau * Q)\n",
    "    denominator = math.exp(tau * Q) + math.exp(tau * Qalt)\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Initialize transition and reward matrices\n",
    "\n",
    "For model-based learning, the model must learning the probability of transitioning between states (in a transition matrix $T$) and the expectation of receiving reward in terminal states, $R_{exp}$. <b>Create two matrices, <code>T</code> and <code>Rexp</code> of size <code>3x2x3</code> and <code>3x2</code>, respectively, with initial values for each matrix element drawn from a uniform distribution from 0 to 1.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Define the algorithm to calculate $Q_{fwd}$ values\n",
    "\n",
    "$Q$ values in model-based algorithms are calculated recursively as the weighted sum of next step $Q$ values, where weights are determined by the transition matrix $T$. For the two-stage task, we can program this algorithm in a relatively simple manner since (1) $Q$ values at the first stage are determined by second stage $Q$ values alone, and (2) $Q$ values at the second stage are determined by <code>Rexp</code>. \n",
    "\n",
    "<b>Write an algorithm to calculate $Q_{fwd}$ such that\n",
    "\n",
    "    - if value is requested from stage 1 (i.e. s==0), then a value is returned that depends on T, but\n",
    "    - if value is requested from stage 2 (i.e. s>0), then a value from Rexp is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Qfwd(s,a):\n",
    "    # if calculating Qfwd from stage 1 then sum over possible second stage states \n",
    "    # weighted by transition probability\n",
    "\n",
    "    \n",
    "    # otherwise, actions give rewards and then the trial ends\n",
    "\n",
    "    \n",
    "print(Qfwd(0,LEFT), Qfwd(0,RIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Program the model to perform the task\n",
    "It will be necessary to calculate a reward prediction error for the transition from stage 2 to reward receipt. This prediction error should be determine by \n",
    "\n",
    "$$ \\delta_r = r - R_{exp}(s, a) $$\n",
    "\n",
    "where $r$ is the reward earned on the trial and $s$, $a$ are the second stage state and action. Update $R_{exp}(s,a)$ according to\n",
    "\n",
    "$$ R_{exp}(s_a) \\leftarrow R_{exp}(s,a) + \\eta \\delta_r.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the learning rate used for updating the transition matrix\n",
    "# and reward expectation\n",
    "eta = .3\n",
    "\n",
    "# store stage 2 actions for plotting\n",
    "stage2_actions = []\n",
    "\n",
    "\n",
    "for trial in range(300):\n",
    "    '''\n",
    "    STAGE 1: determine action taken, whether the transition was \n",
    "    common or rare, and what the next state is.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Update the transition matrix, T, based on transition from\n",
    "    stage 1 to stage 2. Don't forget to update the value for\n",
    "    the stage 2 state not visited.\n",
    "    '''\n",
    "\n",
    "        \n",
    "    '''\n",
    "    STAGE 2: determine action taken and whether reward was earned.\n",
    "    Store stage 2 state in variable s2 (value of 1 or 2) and stage\n",
    "    2 action in variable a2 (value of LEFT or RIGHT)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Update reward estimation matrix, Rexp, based on reward earned\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Store action taken on stage 2\n",
    "    '''\n",
    "    stage2_actions.append(2*(s2-1)+a2) # converts state-actions to (0,1,2,3)\n",
    "\n",
    "    \n",
    "'''\n",
    "Plot the model performance\n",
    "'''\n",
    "plt.rcParams['figure.figsize'] = (5,7)\n",
    "axis1 = plt.subplot(2,1,1)\n",
    "a10 = np.add(np.where(np.asarray(stage2_actions)==0),1)\n",
    "a11 = np.add(np.where(np.asarray(stage2_actions)==1),1)\n",
    "a20 = np.add(np.where(np.asarray(stage2_actions)==2),1)\n",
    "a21 = np.add(np.where(np.asarray(stage2_actions)==3),1)\n",
    "axis1.plot(a10, 0*np.divide(a10,a10), 'b.')\n",
    "axis1.plot(a11, 1*np.divide(a11,a11), 'g.')\n",
    "axis1.plot(a20, 2*np.divide(a20,a20), 'r.')\n",
    "axis1.plot(a21, 3*np.divide(a21,a21), 'c.')\n",
    "plt.yticks((0,1,2,3), ('1,L', '1,R', '2,L', '2,R'))\n",
    "plt.ylim(-.5,3.5)\n",
    "plt.ylabel('Stage 2 Action')\n",
    "plt.title('Actions with Drifting Reward Probabilities')\n",
    "\n",
    "axis2 = plt.subplot(2,1,2)\n",
    "axis2.plot(range(1,301), R[1,0,:], 'b-')\n",
    "axis2.plot(range(1,301), R[1,1,:], 'g-')\n",
    "axis2.plot(range(1,301), R[2,0,:], 'r-')\n",
    "axis2.plot(range(1,301), R[2,1,:], 'c-')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Probability of Reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "<b>Copy and modify your code from Part D to reproduce Figure 1C from Otto et al.</b> \n",
    "\n",
    "To do this you need to store the probabilities of repeating the action taken in stage 1 based on whether the transition was <code>COMMON</code> or <code>RARE</code> and whether reward was earned on stage 2. Store these probabilities in the four arrays initialized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the learning rate used for updating the transition matrix\n",
    "# and reward expectation\n",
    "eta = .3\n",
    "\n",
    "stay_common_reward = []\n",
    "stay_rare_reward = []\n",
    "stay_common_noreward = []\n",
    "stay_rare_noreward = []\n",
    "\n",
    "for trial in range(300):\n",
    "    '''\n",
    "    STAGE 1: determine action taken, whether the transition was \n",
    "    common or rare, and what the next state is.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Update the transition matrix, T, based on transition from\n",
    "    stage 1 to stage 2\n",
    "    '''\n",
    "\n",
    "        \n",
    "    '''\n",
    "    STAGE 2: determine action taken and whether reward was earned.\n",
    "    Store stage 2 state in variable s2 (value of 1 or 2) and stage\n",
    "    2 action in variable a2 (value of LEFT or RIGHT)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Update reward estimation matrix, Rexp, based on reward earned\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Store stay probabilities\n",
    "    '''\n",
    "\n",
    "            \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "(figure, axis) = plt.subplots()\n",
    "axis.bar((1,3), (np.mean(stay_common_reward), np.mean(stay_common_noreward)), .75, color='blue')\n",
    "axis.bar((2,4), (np.mean(stay_rare_reward), np.mean(stay_rare_noreward)), .75, color='red')\n",
    "plt.ylim(.25,1)\n",
    "plt.ylabel('Stay Probability')\n",
    "axis.set_xticklabels(('Rewarded', 'Unrewarded'))\n",
    "axis.set_xticks((1.5, 3.5))\n",
    "plt.legend(('Common', 'Rare'))\n",
    "plt.title('Model Free')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The following code defines and draws a maze. The green circle is the starting position (stored in the tuple <code>maze.start</code>) and the red circle is where reward is delivered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAEyCAYAAABwLfy/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIRD2ACEQ9gAhPO/hG4oPqsZddMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad2e898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Maze:\n",
    "    size_rows = 5\n",
    "    size_cols = 5\n",
    "    \n",
    "    start = (0,0)\n",
    "    \n",
    "    # 4 possible actions: 0-UP, 1-RIGHT, 2-DOWN, 3-LEFT\n",
    "    actions = np.zeros( (size_rows, size_cols, 4) )\n",
    "    # reward for each state is defined in init_rewards\n",
    "    reward = np.zeros( (size_rows, size_cols) )\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        action_array = self.actions[state[0], state[1]]\n",
    "        return [i for i in range(len(action_array)) if action_array[i]]\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        return self.reward[state[0], state[1]]\n",
    "    \n",
    "    def is_terminal_state(self, state):\n",
    "        t = [(0,2), (1,0), (1,2), (0,3), (3,2)]\n",
    "        t += [(4,0), (4,4)]\n",
    "        \n",
    "        if state in t:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def next_state(self, state, action):\n",
    "        a = self.get_actions((state[0], state[1]))\n",
    "        if action in a:\n",
    "            state_delta = [ (0,1), (1,0), (0,-1), (-1,0) ]\n",
    "            return (state[0] + state_delta[action][0], state[1] + state_delta[action][1])\n",
    "        else:\n",
    "            return state\n",
    "        \n",
    "    def draw_with_path(self, path, title=''):\n",
    "        plt.rcParams['figure.figsize'] = (5,5)\n",
    "        (figure, axis) = plt.subplots()\n",
    "        for i in range(self.size_rows):\n",
    "            for j in range(self.size_cols):\n",
    "                if (i,j) == self.start:\n",
    "                    circle = plt.Circle((i+.5, j+.5), radius=0.25, fc='g')\n",
    "                    axis.add_patch(circle)\n",
    "                    \n",
    "                if self.reward[i,j]:\n",
    "                    circle = plt.Circle((i+.5, j+.5), radius=0.25, fc='r')\n",
    "                    axis.add_patch(circle)\n",
    "                \n",
    "                a = self.actions[i,j]\n",
    "                if not a[0]: # UP\n",
    "                    axis.plot([i, i+1], [j+1, j+1], 'k-')\n",
    "                if not a[1]: # RIGHT\n",
    "                    axis.plot([i+1,i+1], [j, j+1], 'k-')\n",
    "                if not a[2]: # DOWN\n",
    "                    axis.plot([i,i+1], [j, j], 'k-')\n",
    "                if not a[3]: # LEFT\n",
    "                    axis.plot([i,i], [j, j+1], 'k-')\n",
    "        \n",
    "        plt.Circle((.5, .5), radius=0.25, fc='g')\n",
    "        \n",
    "        r1 = (0.5,0.5)\n",
    "        for i in range(1, len(path)):\n",
    "            r2 = np.random.uniform(.3,.7,2) if i<len(path)-1 else (.5,.5)\n",
    "            s1 = np.add(path[i-1], r1)\n",
    "            s2 = np.add(path[i], r2)\n",
    "            r1 = r2\n",
    "            \n",
    "            axis.plot([s1[0], s2[0]], [s1[1], s2[1]], 'm-.')\n",
    "        \n",
    "        if len(title):\n",
    "            plt.title(title)\n",
    "        \n",
    "        plt.show()\n",
    "      \n",
    "    def draw(self):\n",
    "        self.draw_with_path([])\n",
    "    \n",
    "    def init_actions(self):\n",
    "        # first, make all actions illegal\n",
    "        self.actions = np.zeros( (self.size_rows,self.size_cols,4) )\n",
    "        \n",
    "        # now let's draw a maze\n",
    "        self.actions[0,0,0] = 1\n",
    "        self.actions[0,1,2] = 1\n",
    "        self.actions[0,1,0] = 1\n",
    "        self.actions[0,1,1] = 1\n",
    "        self.actions[0,2,2] = 1\n",
    "        self.actions[1,1,3] = 1\n",
    "        self.actions[1,1,1] = 1\n",
    "        self.actions[2,1,3] = 1\n",
    "        self.actions[2,1,1] = 1\n",
    "        self.actions[3,1,3] = 1\n",
    "        self.actions[3,1,2] = 1\n",
    "        self.actions[3,0,0] = 1\n",
    "        self.actions[3,0,3] = 1\n",
    "        self.actions[2,0,1] = 1\n",
    "        self.actions[2,0,3] = 1\n",
    "        self.actions[1,0,1] = 1\n",
    "        self.actions[2,1,0] = 1\n",
    "        self.actions[2,2,2] = 1\n",
    "        self.actions[2,2,0] = 1\n",
    "        self.actions[2,3,2] = 1\n",
    "        self.actions[2,3,1] = 1\n",
    "        self.actions[2,3,3] = 1\n",
    "        self.actions[1,3,1] = 1\n",
    "        self.actions[1,3,2] = 1\n",
    "        self.actions[1,2,0] = 1\n",
    "        self.actions[3,3,3] = 1\n",
    "        self.actions[3,3,1] = 1\n",
    "        self.actions[4,3,3] = 1\n",
    "        self.actions[4,3,2] = 1\n",
    "        self.actions[4,2,0] = 1\n",
    "        self.actions[4,2,3] = 1\n",
    "        self.actions[3,2,1] = 1\n",
    "        self.actions[4,2,2] = 1\n",
    "        self.actions[4,1,0] = 1\n",
    "        self.actions[4,1,2] = 1\n",
    "        self.actions[4,0,0] = 1\n",
    "        self.actions[1,3,0] = 1\n",
    "        self.actions[1,4,2] = 1\n",
    "        self.actions[1,4,3] = 1\n",
    "        self.actions[0,4,1] = 1\n",
    "        self.actions[0,4,2] = 1\n",
    "        self.actions[0,3,0] = 1\n",
    "        self.actions[1,4,1] = 1\n",
    "        self.actions[2,4,3] = 1\n",
    "        self.actions[2,4,1] = 1\n",
    "        self.actions[3,4,3] = 1\n",
    "        self.actions[3,4,1] = 1\n",
    "        self.actions[4,4,3] = 1\n",
    "    \n",
    "    def init_rewards(self):\n",
    "        self.reward = np.zeros( (self.size_rows, self.size_cols) )\n",
    "        self.reward[self.size_rows-1,self.size_cols-1] = 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.init_actions()\n",
    "        self.init_rewards()\n",
    "        \n",
    "maze = Maze()\n",
    "maze.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse temperature – adjust this as you see fit\n",
    "tau = 10\n",
    "\n",
    "def Ps(Q, Qall):\n",
    "    global tau\n",
    "    numerator = math.exp(tau * Q)\n",
    "    \n",
    "    denominator = 0\n",
    "    for i in range(len(Qall)):\n",
    "        denominator += math.exp(tau * Qall[i])\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Generally, we would begin by initializing a transition matrix, $T$, and a matrix of the expected reward at each state, $R_{exp}$. However, for simplicity, we can take advantage of the fact that state transitions are deterministic and that the maze returns the state $s'$ reached when taking action $a$ in state $s$ using <code>s_prime = maze.next_state(s, a)</code>.\n",
    "\n",
    "However, let's not assume that the model knows the rewards that are available in the maze, so that there is <i>something</i> to learn. <b>Initialize <code>Rexp</code> as matrix of size <code>5x5</code></b>. As in problem 1, values for elements in the matrices should be drawn from uniform distributions between 0 and 1. The sizes of this matrix is determined by the fact that the maze is 5x5 in size. Each position is represented by a two positions – left-right, and down-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "This is almost exactly the same maze code as in the previous assignment. However, for the sake of tree search efficiency, an additional method has been added to <code>Maze</code> that defines dead end positions in the maze as terminal states. This will be important for you since $Q_{fwd}$ values are calculated by searching forward until a terminal state. $Q_{fwd}$ should therefore be equal to <code>maze.get_reward(s)</code> if <code>maze.is_terminal_state(s)</code> is <code>True</code> or if <code>s==maze.start</code>. Otherwise, $Q_{fwd}$ should be equal to the maximum of $Q_{fwd}$ values for states that are achievable after taking action $a$ in state $s$.\n",
    "\n",
    "<b>Define a <code>Qfwd</code> function for the maze.</b>\n",
    "\n",
    "When calculating the list of actions available after transitioning from $s$ to $s'$ you should ignore actions, $a'$, that return to $s$. Without this, forward search gets stuck in infinite regressive search $s \\rightarrow s' \\rightarrow s \\rightarrow s' \\rightarrow etc.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Qfwd(s,a):\n",
    "    # determine state reached by taking action a in state s\n",
    "\n",
    "    # test if the next state is a terminal state or the start state\n",
    "    # and return value from Rexp if so\n",
    "\n",
    "    \n",
    "    # otherwise, return the maximum Q value achievable from the next state\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a skeleton for a $Q_{fwd}$-learning model in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start at intended beginning location\n",
    "s = maze.start\n",
    "# store all traversed positions in maze in list for drawing at end\n",
    "s_hist = [s]\n",
    "\n",
    "# navigate until reward is reached\n",
    "while not maze.is_terminal_state(s):\n",
    "    # find all possible actions from current position\n",
    "    a_poss = maze.get_actions(s)\n",
    "\n",
    "    # calculate Q-values for all actions from the current state\n",
    "    Qall = [Qfwd(s,a) for a in a_poss]\n",
    "\n",
    "    # convert Q-values into probability of selection using softmax function\n",
    "    Ps_all = [Ps(Q, Qall) for Q in Qall]\n",
    "    \n",
    "    # randomly select action based on probability of selection\n",
    "    a = np.random.choice(a_poss, p=Ps_all)\n",
    "\n",
    "    # determine next state\n",
    "    s_prime = maze.next_state(s, a)\n",
    "    \n",
    "    # reward earned?\n",
    "    if maze.is_terminal_state(s_prime) or s_prime==maze.start:\n",
    "        r = maze.get_reward(s)\n",
    "\n",
    "    # update s and add to list of visited states\n",
    "    s = s_prime\n",
    "    s_hist.append(s)\n",
    "\n",
    "# draw the maze with the random path\n",
    "maze.draw_with_path(s_hist)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write the learning algorithm. \n",
    "\n",
    "Plot the number of actions required to reach the reward on 100 runs through the maze as a function of trial number.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning rate for Rexp\n",
    "eta = 0.5\n",
    "\n",
    "# things to plot\n",
    "step_counts = []\n",
    "rewards_earned = []\n",
    "\n",
    "# reinitialize Rexp\n",
    "\n",
    "\n",
    "for trial in range(100):\n",
    "    num_steps = 0\n",
    "    \n",
    "    s = maze.start\n",
    "    s_hist = [s]\n",
    "    \n",
    "    while not maze.is_terminal_state(s):\n",
    "        \n",
    "        # This is where your algorithm goes. I recommend starting with the algorithm\n",
    "        # structure above.\n",
    "            \n",
    "        # add s to list of visited states\n",
    "        s_hist.append(s)\n",
    "        \n",
    "        num_steps += 1\n",
    "        \n",
    "    if trial == 0:\n",
    "        s_hist_init = s_hist.copy()\n",
    "    \n",
    "    step_counts.append(num_steps)\n",
    "    rewards_earned.append(r)\n",
    "\n",
    "\n",
    "maze.draw_with_path(s_hist_init, 'Initial')\n",
    "maze.draw_with_path(s_hist, 'Final')\n",
    "\n",
    "(figure,axis) = plt.subplots()\n",
    "axis.plot(step_counts)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Steps taken')\n",
    "plt.title('Learning')\n",
    "plt.show()\n",
    "\n",
    "(figure,axis) = plt.subplots()\n",
    "axis.plot(rewards_earned)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Reward earned')\n",
    "plt.title('Learning')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
