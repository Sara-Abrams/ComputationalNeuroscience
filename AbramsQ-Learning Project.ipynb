{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based versus model-free learning project: $Q$-learning\n",
    "<font color=red>Not much here.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Define the task\n",
    "\n",
    "The task is composed of three possible states and two actions per state. Transition to a new state only occurs from the first stage of the task. Write a function <b><code>next_state</code></b> that returns the next state (either 1 or 2) based on the action taken (<code>LEFT</code> or <code>RIGHT</code>) in stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "COMMON = 0\n",
    "RARE = 1\n",
    "\n",
    "def next_state(action):\n",
    "    # write your code here\n",
    "    # return a tuple that is the next state (1 or 2) and whether the\n",
    "    # transition was common or rare.\n",
    "    #    e.g. return (1, COMMON)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Define the rewards\n",
    "\n",
    "Reward earned on the task is determined by the action taken on stage 2 with reward probability consistently changing between .25 and .75 according to Gaussian random walks (with $\\sigma$=.025).\n",
    "\n",
    "Create an array that is of size 3 $\\times$ 2 $\\times$ 300 (number of states $\\times$ number of actions $\\times$ number of trials) where each sub-array, <code>R(s,a,:)</code>, is determined by a Gaussian random walk. If the random walk caused probabilities to leave the [.25, .75] range, then reflect the value around the minimum or maximum. For example, if the random walk would change <code>p</code> to <code>.20</code>, then its new value should be <code>p = .25 + (.25 - .2) = .3</code>. Likewise, is <code>p</code> would change to <code>0.87</code> then its new value should be <code>p = .75 - (.87 - .75) = .63</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = np.zeros( (3,2,300) )\n",
    "\n",
    "# randomly set intial reward probabilities between .25 and .75 for states 1, 2\n",
    "# leave reward for stage 1 (state=0) zero\n",
    "R[1:,:,0] = np.random.rand(2,2) * .5 + .25\n",
    "\n",
    "for trial in range(1, 300):\n",
    "    # write algorithm to create random walk for each state-action\n",
    "    \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n",
    "(figure, axis) = plt.subplots()\n",
    "axis.plot(range(1,301), R[1,0,:], 'b-')\n",
    "axis.plot(range(1,301), R[1,1,:], 'g-')\n",
    "axis.plot(range(1,301), R[2,0,:], 'r-')\n",
    "axis.plot(range(1,301), R[2,1,:], 'c-')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Probability of Reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Softmax decision function\n",
    "\n",
    "Choices in the task are made using a softmax decision function that assigns a probability of selecting an action that depends on the $Q$ value for that action relative to the alternative $Q$ value. Plot the softmax function for $Q$ values ranging from 0 to 1 assuming that the alternative action has $Q=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inverse temperature parameter\n",
    "tau = 10\n",
    "\n",
    "def softmax(Q, Qalt):\n",
    "    # write algorithm to calculate softmax probability\n",
    "\n",
    "Qlist = np.arange(0, 1.05, .05)\n",
    "\n",
    "# create a list named Ps that give the choice probability for each Q value in Qlist\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "(figure, axis) = plt.subplots()\n",
    "axis.plot(Qlist, Ps, 'k-')\n",
    "axis.plot([0,1], [.5,.5], 'k:', linewidth=.25)\n",
    "axis.plot([.5,.5], [1,0], 'k:', linewidth=.25)\n",
    "plt.xlabel('Q')\n",
    "plt.ylabel('Softmax Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: SARSA algorithm\n",
    "\n",
    "In order to reproduce the results in Otto et al. an additional element must be added to the SARSA algorithm described in Glasher et al. Specifically, $Q$ values from stage 1 must be updated based on prediction errors calculated on stage 2. This is a common feature of reinforcement learning algorithms that allows prior state-actions to gain some credit for reward earned later on.\n",
    "\n",
    "Of course, early actions should not necessarily receive full credit for reward earned later. The way this is implemented in reinforcement learning is to define a separate variable $\\lambda$ (called the <b>eligibility trace</b> such that prior actions are updated proportional to $\\lambda^t$ for prediction errors occurring $t$ time steps after the action was taken. For the two stage task, this implies that stage 1 state-actions should be increased by $\\alpha \\times \\lambda \\times \\delta$ where $\\alpha$ is the learning rate and $\\delta$ is the prediction error calculated on stage 2. Otto et al. used $\\lambda=1$ (i.e. full credit for stage 1 actions based on reward earned in stage 2).\n",
    "\n",
    "<b>Complete the following code to implement a SARSA learning algorithm.</b> I find it easiest to code this model by first determining first and second stage states and actions, followed by Q value updating based on prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elig_trace = 1 # i.e. lambda\n",
    "gamma = 1\n",
    "alpha = .2\n",
    "tau = 5\n",
    "\n",
    "# store stage 2 actions for plotting\n",
    "stage2_actions = []\n",
    "\n",
    "# initialize Q values\n",
    "Q = np.zeros( (3,2) ) # i.e. number of states x number of actions\n",
    "\n",
    "for trial in range(300):\n",
    "    '''\n",
    "    STAGE 1: determine action taken, whether the transition was \n",
    "    common or rare, and what the next state is.\n",
    "    '''\n",
    "\n",
    "        \n",
    "    '''\n",
    "    STAGE 2: determine action taken and whether reward was earned.\n",
    "    Store stage 2 state in variable s2 (value of 1 or 2) and stage\n",
    "    2 action in variable a2 (value of LEFT or RIGHT)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    STAGE 1 value update: determine prediction error and update\n",
    "    Q value for transition from stage 1 to stage 2\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    STAGE 2 value update: determine prediction error and update\n",
    "    Q values for transition from stage 1 to stage 2. Recall that\n",
    "    both stage 1 and stage 2 Q values must be updated based on\n",
    "    stage 2 prediction error\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Store action taken on stage 2\n",
    "    '''\n",
    "    stage2_actions.append(2*(s2-1)+a2) # converts state-actions to (0,1,2,3)\n",
    "    \n",
    "'''\n",
    "Plot the model performance\n",
    "'''\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "axis1 = plt.subplot(2,1,1)\n",
    "a10 = np.add(np.where(np.asarray(stage2_actions)==0),1)\n",
    "a11 = np.add(np.where(np.asarray(stage2_actions)==1),1)\n",
    "a20 = np.add(np.where(np.asarray(stage2_actions)==2),1)\n",
    "a21 = np.add(np.where(np.asarray(stage2_actions)==3),1)\n",
    "axis1.plot(a10, 0*np.divide(a10,a10), 'b.')\n",
    "axis1.plot(a11, 1*np.divide(a11,a11), 'g.')\n",
    "axis1.plot(a20, 2*np.divide(a20,a20), 'r.')\n",
    "axis1.plot(a21, 3*np.divide(a21,a21), 'c.')\n",
    "plt.yticks((0,1,2,3), ('1,L', '1,R', '2,L', '2,R'))\n",
    "plt.ylim(-.5,3.5)\n",
    "plt.ylabel('Stage 2 Action')\n",
    "plt.title('Actions with Drifting Reward Probabilities')\n",
    "\n",
    "axis2 = plt.subplot(2,1,2)\n",
    "axis2.plot(range(1,301), R[1,0,:], 'b-')\n",
    "axis2.plot(range(1,301), R[1,1,:], 'g-')\n",
    "axis2.plot(range(1,301), R[2,0,:], 'r-')\n",
    "axis2.plot(range(1,301), R[2,1,:], 'c-')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Probability of Reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "<b>Copy and modify your code from Part D to reproduce Figure 1B from Otto et al.</b> \n",
    "\n",
    "To do this you need to store the probabilities of repeating the action taken in stage 1 based on whether the transition was <code>COMMON</code> or <code>RARE</code> and whether reward was earned on stage 2. Store these probabilities in the four arrays initialized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stay_common_reward = []\n",
    "stay_rare_reward = []\n",
    "stay_common_noreward = []\n",
    "stay_rare_noreward = []\n",
    "\n",
    "elig_trace = 1 # i.e. lambda\n",
    "gamma = 1\n",
    "alpha = .2\n",
    "tau = 5\n",
    "\n",
    "# initialize Q values\n",
    "Q = np.zeros( (3,2) ) # i.e. number of states x number of actions\n",
    "\n",
    "for trial in range(300):\n",
    "    '''\n",
    "    STAGE 1: determine action taken, whether the transition was \n",
    "    common or rare, and what the next state is.\n",
    "    '''\n",
    "\n",
    "        \n",
    "    '''\n",
    "    STAGE 2: determine action taken and whether reward was earned.\n",
    "    Store stage 2 state in variable s2 (value of 1 or 2) and stage\n",
    "    2 action in variable a2 (value of LEFT or RIGHT)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    STAGE 1 value update: determine prediction error and update\n",
    "    Q value for transition from stage 1 to stage 2\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    STAGE 2 value update: determine prediction error and update\n",
    "    Q values for transition from stage 1 to stage 2. Recall that\n",
    "    both stage 1 and stage 2 Q values must be updated based on\n",
    "    stage 2 prediction error\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Store stay probabilities in each of the 4 arrays \n",
    "    stay_common_reward, stay_rare_reward, stay_common_noreward, stay_rare_noreward\n",
    "    '''\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "(figure, axis) = plt.subplots()\n",
    "axis.bar((1,3), (np.mean(stay_common_reward), np.mean(stay_common_noreward)), .75, color='blue')\n",
    "axis.bar((2,4), (np.mean(stay_rare_reward), np.mean(stay_rare_noreward)), .75, color='red')\n",
    "plt.ylim(.25,1)\n",
    "plt.ylabel('Stay Probability')\n",
    "axis.set_xticklabels(('Rewarded', 'Unrewarded'))\n",
    "axis.set_xticks((1.5, 3.5))\n",
    "plt.legend(('Common', 'Rare'))\n",
    "plt.title('Model Free')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The following code defines and draws a maze. The green circle is the starting position (stored in the tuple <code>maze.start</code>) and the red circle is where reward is – and where running through the maze is terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    size_rows = 5\n",
    "    size_cols = 5\n",
    "    \n",
    "    start = (0,0)\n",
    "    \n",
    "    # 4 possible actions: 0-UP, 1-RIGHT, 2-DOWN, 3-LEFT\n",
    "    actions = np.zeros( (size_rows, size_cols, 4) )\n",
    "    # reward for each state is defined in init_rewards\n",
    "    reward = np.zeros( (size_rows, size_cols) )\n",
    "    \n",
    "    def get_actions(self, state):\n",
    "        action_array = self.actions[state[0], state[1]]\n",
    "        return [i for i in range(len(action_array)) if action_array[i]]\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        return self.reward[state[0], state[1]]\n",
    "    \n",
    "    def next_state(self, state, action):\n",
    "        a = self.get_actions((state[0], state[1]))\n",
    "        if action in a:\n",
    "            state_delta = [ (0,1), (1,0), (0,-1), (-1,0) ]\n",
    "            return (state[0] + state_delta[action][0], state[1] + state_delta[action][1])\n",
    "        else:\n",
    "            return state\n",
    "        \n",
    "    def draw_with_path(self, path, title=''):\n",
    "        plt.rcParams['figure.figsize'] = (5,5)\n",
    "        (figure, axis) = plt.subplots()\n",
    "        for i in range(self.size_rows):\n",
    "            for j in range(self.size_cols):\n",
    "                if (i,j) == self.start:\n",
    "                    circle = plt.Circle((i+.5, j+.5), radius=0.25, fc='g')\n",
    "                    axis.add_patch(circle)\n",
    "                    \n",
    "                if self.reward[i,j]:\n",
    "                    circle = plt.Circle((i+.5, j+.5), radius=0.25, fc='r')\n",
    "                    axis.add_patch(circle)\n",
    "                \n",
    "                a = self.actions[i,j]\n",
    "                if not a[0]: # UP\n",
    "                    axis.plot([i, i+1], [j+1, j+1], 'k-')\n",
    "                if not a[1]: # RIGHT\n",
    "                    axis.plot([i+1,i+1], [j, j+1], 'k-')\n",
    "                if not a[2]: # DOWN\n",
    "                    axis.plot([i,i+1], [j, j], 'k-')\n",
    "                if not a[3]: # LEFT\n",
    "                    axis.plot([i,i], [j, j+1], 'k-')\n",
    "        \n",
    "        plt.Circle((.5, .5), radius=0.25, fc='g')\n",
    "        \n",
    "        r1 = (0.5,0.5)\n",
    "        for i in range(1, len(path)):\n",
    "            r2 = np.random.uniform(.3,.7,2) if i<len(path)-1 else (.5,.5)\n",
    "            s1 = np.add(path[i-1], r1)\n",
    "            s2 = np.add(path[i], r2)\n",
    "            r1 = r2\n",
    "            \n",
    "            axis.plot([s1[0], s2[0]], [s1[1], s2[1]], 'm-.')\n",
    "        \n",
    "        if len(title):\n",
    "            plt.title(title)\n",
    "        \n",
    "        plt.show()\n",
    "      \n",
    "    def draw(self):\n",
    "        self.draw_with_path([])\n",
    "    \n",
    "    def init_actions(self):\n",
    "        # first, make all actions illegal\n",
    "        self.actions = np.zeros( (self.size_rows,self.size_cols,4) )\n",
    "        \n",
    "        # now let's draw a maze\n",
    "        self.actions[0,0,0] = 1\n",
    "        self.actions[0,1,2] = 1\n",
    "        self.actions[0,1,0] = 1\n",
    "        self.actions[0,1,1] = 1\n",
    "        self.actions[0,2,2] = 1\n",
    "        self.actions[1,1,3] = 1\n",
    "        self.actions[1,1,1] = 1\n",
    "        self.actions[2,1,3] = 1\n",
    "        self.actions[2,1,1] = 1\n",
    "        self.actions[3,1,3] = 1\n",
    "        self.actions[3,1,2] = 1\n",
    "        self.actions[3,0,0] = 1\n",
    "        self.actions[3,0,3] = 1\n",
    "        self.actions[2,0,1] = 1\n",
    "        self.actions[2,0,3] = 1\n",
    "        self.actions[1,0,1] = 1\n",
    "        self.actions[2,1,0] = 1\n",
    "        self.actions[2,2,2] = 1\n",
    "        self.actions[2,2,0] = 1\n",
    "        self.actions[2,3,2] = 1\n",
    "        self.actions[2,3,1] = 1\n",
    "        self.actions[2,3,3] = 1\n",
    "        self.actions[1,3,1] = 1\n",
    "        self.actions[1,3,2] = 1\n",
    "        self.actions[1,2,0] = 1\n",
    "        self.actions[3,3,3] = 1\n",
    "        self.actions[3,3,1] = 1\n",
    "        self.actions[4,3,3] = 1\n",
    "        self.actions[4,3,2] = 1\n",
    "        self.actions[4,2,0] = 1\n",
    "        self.actions[4,2,3] = 1\n",
    "        self.actions[3,2,1] = 1\n",
    "        self.actions[4,2,2] = 1\n",
    "        self.actions[4,1,0] = 1\n",
    "        self.actions[4,1,2] = 1\n",
    "        self.actions[4,0,0] = 1\n",
    "        self.actions[1,3,0] = 1\n",
    "        self.actions[1,4,2] = 1\n",
    "        self.actions[1,4,3] = 1\n",
    "        self.actions[0,4,1] = 1\n",
    "        self.actions[0,4,2] = 1\n",
    "        self.actions[0,3,0] = 1\n",
    "        self.actions[1,4,1] = 1\n",
    "        self.actions[2,4,3] = 1\n",
    "        self.actions[2,4,1] = 1\n",
    "        self.actions[3,4,3] = 1\n",
    "        self.actions[3,4,1] = 1\n",
    "        self.actions[4,4,3] = 1\n",
    "    \n",
    "    def init_rewards(self):\n",
    "        self.reward = np.zeros( (self.size_rows, self.size_cols) )\n",
    "        self.reward[self.size_rows-1,self.size_cols-1] = 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.init_actions()\n",
    "        self.init_rewards()\n",
    "        \n",
    "maze = Maze()\n",
    "maze.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maze code comes with a few functions that should hopefully make writing navigation code easier. The position within the maze should be stored as a tuple where <code>(0,0)</code> is the bottom left corner, moving to the right increases the first element of the tuple, and moving up increases the second element of the tuple. If you initialize a Maze object with\n",
    "\n",
    "<code>maze = Maze()</code>\n",
    "\n",
    "then <code>maze.start</code> gives you the intended start position within the maze.\n",
    "\n",
    "For any state, <code>s</code>, within the maze, the function <code>maze.get_actions()</code> returns a list of all possible actions available from that state (i.e. the walls of the maze may not be crossed and hence represent illegal actions). You can determine the next state reached for action <code>a</code> in state <code>s</code> using <code>maze.next_state(s,a)</code>. The reward available in a state is obtained using <code>maze.get_reward(s)</code>.\n",
    "\n",
    "To see how this all works, let's begin be defining a softmax decision function (see Glascher et al., 2010):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inverse temperature – adjust this as you see fit\n",
    "tau = 10\n",
    "\n",
    "def Ps(Q, Qall):\n",
    "    numerator = math.exp(tau * Q)\n",
    "    \n",
    "    denominator = 0\n",
    "    for i in range(len(Qall)):\n",
    "        denominator += math.exp(tau * Qall[i])\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a skeleton for a $Q$-learning model (that does not learn!) in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store Q-values based on x position, y position, and possible directions of movement\n",
    "Q = np.zeros( (maze.size_rows, maze.size_cols, 4) )\n",
    "\n",
    "# start at intended beginning location\n",
    "s = maze.start\n",
    "# store all traversed positions in maze in list for drawing at end\n",
    "s_hist = [s]\n",
    "\n",
    "# navigate until reward is reached\n",
    "while not maze.get_reward(s):\n",
    "    # find all possible actions from current position\n",
    "    a_poss = maze.get_actions(s)\n",
    "\n",
    "    # calculate Q-values for all actions from the current state\n",
    "    Qall = [Q[s[0], s[1], a] for a in a_poss]\n",
    "\n",
    "    # convert Q-values into probability of selection using softmax function\n",
    "    Ps_all = [Ps(Q, Qall) for Q in Qall]\n",
    "    \n",
    "    # randomly select action based on probability of selection\n",
    "    Ps_cumsum = np.cumsum(Ps_all)\n",
    "    rand = np.random.uniform()\n",
    "    for i in range(len(a_poss)):\n",
    "        if rand <= Ps_cumsum[i]:\n",
    "            a = a_poss[i]\n",
    "            break\n",
    "\n",
    "    # traverse to next state\n",
    "    s = maze.next_state(s, a)\n",
    "    # reward earned?\n",
    "    r = maze.get_reward(s)\n",
    "\n",
    "    # add new state to list of visited states\n",
    "    s_hist.append(s)\n",
    "\n",
    "# draw the maze with the random path\n",
    "maze.draw_with_path(s_hist)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Write a SARSA algorithm that learns to navigate the maze. I found that using a learning rate of <code>alpha=0.3</code> and discount rate of <code>gamma=0.9</code> allows the maze to be learned in under 100 trials.\n",
    "\n",
    "Recall that the prediction error in the SARSA algorithm is given by\n",
    "\n",
    "$$\\delta(s) = r(s') + \\gamma Q(s',a') - Q(s,a)$$\n",
    "\n",
    "where $s'$ and $a'$ are the state and action taken after action $a$ is taken in state $s$.\n",
    "\n",
    "<b>Plot the number of actions required to reach the reward on 100 runs through the maze as a function of trial number.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = np.zeros( (maze.size_rows, maze.size_cols, 4) )\n",
    "eta = 0.3\n",
    "gamma = .9\n",
    "\n",
    "step_counts = []\n",
    "\n",
    "for trial in range(100):\n",
    "    num_steps = 0\n",
    "    \n",
    "    s = maze.start\n",
    "    s_hist = []\n",
    "    \n",
    "    while not maze.get_reward(s) and num_steps<500:\n",
    "        num_steps += 1\n",
    "        s_hist.append(s)\n",
    "        \n",
    "        # SARSA algorithm!\n",
    "    \n",
    "    s_hist.append(s)\n",
    "    \n",
    "    if trial == 0:\n",
    "        s_hist_init = s_hist.copy()\n",
    "    \n",
    "    step_counts.append(num_steps)\n",
    "\n",
    "\n",
    "maze.draw_with_path(s_hist_init, 'Initial')\n",
    "maze.draw_with_path(s_hist, 'Final')\n",
    "(figure,axis) = plt.subplots()\n",
    "axis.plot(step_counts)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Steps to reward')\n",
    "plt.title('Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part B\n",
    "\n",
    "Repeat Part A except using the Q-learning algorithm.\n",
    "\n",
    "Recall that the prediction error in the Q-learning algorithm is given by\n",
    "\n",
    "$$ \\delta(s) = r(s') + \\max_{a'} \\gamma Q(s',a') - Q(s,a). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = np.zeros( (maze.size_rows, maze.size_cols, 4) )\n",
    "eta = 0.3\n",
    "gamma = .9\n",
    "\n",
    "step_counts = []\n",
    "\n",
    "for trial in range(100):\n",
    "    num_steps = 0\n",
    "    \n",
    "    s = maze.start\n",
    "    s_hist = []\n",
    "    \n",
    "    while not maze.get_reward(s) and num_steps<500:\n",
    "        num_steps += 1\n",
    "        s_hist.append(s)\n",
    "        \n",
    "        # Q-learning algorithm!\n",
    "    \n",
    "    s_hist.append(s)\n",
    "    \n",
    "    if trial == 0:\n",
    "        s_hist_init = s_hist.copy()\n",
    "    \n",
    "    step_counts.append(num_steps)\n",
    "\n",
    "\n",
    "maze.draw_with_path(s_hist_init, 'Initial')\n",
    "maze.draw_with_path(s_hist, 'Final')\n",
    "(figure,axis) = plt.subplots()\n",
    "axis.plot(step_counts)\n",
    "plt.xlabel('Trial #')\n",
    "plt.ylabel('Steps to reward')\n",
    "plt.title('Learning')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
